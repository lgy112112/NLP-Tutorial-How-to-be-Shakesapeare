{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current directory before change: /teamspace/studios/this_studio\n",
      "Current directory after change: /teamspace/studios/this_studio/NLP-Tutorial-How-to-be-Shakesapeare/Its-a-Long-Story\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# è·å–å½“å‰å·¥ä½œç›®å½•\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current directory before change: {current_directory}\")\n",
    "\n",
    "# è¦æ›´æ”¹çš„ç›®æ ‡ç›®å½•\n",
    "target_directory = 'NLP-Tutorial-How-to-be-Shakesapeare/Its-a-Long-Story'\n",
    "\n",
    "# æ›´æ”¹å½“å‰å·¥ä½œç›®å½•\n",
    "os.chdir(target_directory)\n",
    "\n",
    "# è·å–æ›´æ”¹åçš„å½“å‰å·¥ä½œç›®å½•åœ°å€\n",
    "new_directory = os.getcwd()\n",
    "print(f\"Current directory after change: {new_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed data from tokenized_datasets...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk, DatasetDict\n",
    "\n",
    "# åŠ è½½é¢„å¤„ç†åçš„æ•°æ®é›†è·¯å¾„\n",
    "preprocessed_data_path = 'tokenized_datasets'\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å­˜åœ¨å·²ä¿å­˜çš„é¢„å¤„ç†åçš„æ•°æ®é›†\n",
    "if os.path.exists(preprocessed_data_path):\n",
    "    print(f\"Loading preprocessed data from {preprocessed_data_path}...\")\n",
    "    tokenized_datasets = load_from_disk(preprocessed_data_path)\n",
    "\n",
    "    # é€‰æ‹©è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„å„10000æ¡æ•°æ®\n",
    "    tokenized_datasets['train'] = tokenized_datasets['train'].select(range(5000))\n",
    "    tokenized_datasets['validation'] = tokenized_datasets['validation'].select(range(200))\n",
    "    tokenized_datasets['test'] = tokenized_datasets['test'].select(range(200))\n",
    "\n",
    "else:\n",
    "    print(\"Preprocessing data...\")\n",
    "\n",
    "    # åŠ è½½åŸå§‹æ•°æ®é›†\n",
    "    dataset = load_from_disk('filtered_dataset')\n",
    "\n",
    "    # åŠ è½½BARTåˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "    model_name = 'facebook/bart-large-cnn'\n",
    "    tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "    model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "    # å®šä¹‰æ•°æ®é¢„å¤„ç†å‡½æ•°ï¼Œå°†æ•°æ®è½¬æ¢ä¸ºæ¨¡å‹æ‰€éœ€æ ¼å¼\n",
    "    def preprocess_function(examples):\n",
    "        inputs = examples['article']\n",
    "        model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "\n",
    "        # è®¾ç½®ç›®æ ‡ (Target) ä¸º highlights\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['highlights'], max_length=128, truncation=True)\n",
    "\n",
    "        model_inputs['labels'] = labels['input_ids']\n",
    "        return model_inputs\n",
    "\n",
    "    # ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œ map æ“ä½œ\n",
    "    tokenized_datasets = dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=4,  # ä½¿ç”¨çš„è¿›ç¨‹æ•°é‡ï¼Œæ ¹æ®ä½ çš„CPUæ ¸å¿ƒæ•°è°ƒæ•´\n",
    "        remove_columns=['article', 'highlights', 'id']\n",
    "    )\n",
    "\n",
    "    # ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®é›†\n",
    "    tokenized_datasets.save_to_disk(preprocessed_data_path)\n",
    "    print(f\"Preprocessed data saved to {preprocessed_data_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-en-de and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of FSMTForConditionalGeneration were not initialized from the model checkpoint at facebook/wmt19-de-en and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import nlpaug.augmenter.word as naw\n",
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments, BartForConditionalGeneration, BartTokenizer\n",
    "import sacremoses\n",
    "\n",
    "# è®¾ç½® nltk æ•°æ®ä¸‹è½½è·¯å¾„\n",
    "nltk_data_path = 'nltk_data'\n",
    "nltk.data.path.append(nltk_data_path)\n",
    "\n",
    "# ä¸‹è½½æ‰€éœ€çš„ nltk æ•°æ®é›†\n",
    "nltk.download('wordnet', download_dir=nltk_data_path)\n",
    "nltk.download('averaged_perceptron_tagger', download_dir=nltk_data_path)\n",
    "\n",
    "# åˆå§‹åŒ–å¢å¼ºå™¨\n",
    "synonym_aug = naw.SynonymAug(aug_src='wordnet')\n",
    "random_insert_aug = naw.ContextualWordEmbsAug(action=\"insert\", model_path='bert-base-uncased')\n",
    "random_swap_aug = naw.RandomWordAug(action=\"swap\")\n",
    "random_delete_aug = naw.RandomWordAug(action=\"delete\")\n",
    "back_translation_aug = naw.BackTranslationAug(from_model_name='facebook/wmt19-en-de', to_model_name='facebook/wmt19-de-en')\n",
    "\n",
    "# å®šä¹‰å¢å¼ºå‡½æ•°\n",
    "def augment_text(text):\n",
    "    aug_methods = [\n",
    "        synonym_aug,\n",
    "        random_insert_aug,\n",
    "        random_swap_aug,\n",
    "        random_delete_aug,\n",
    "        back_translation_aug\n",
    "    ]\n",
    "    \n",
    "    # 50%çš„æ¦‚ç‡è¿›è¡Œæ•°æ®å¢å¼º\n",
    "    if random.random() < 0.5:\n",
    "        aug_method = random.choice(aug_methods)\n",
    "        text = aug_method.augment(text)\n",
    "    return text\n",
    "\n",
    "class DataCollatorForSeq2SeqWithAugmentation(DataCollatorForSeq2Seq):\n",
    "    def __call__(self, features):\n",
    "        for feature in features:\n",
    "            if 'article' in feature:\n",
    "                feature['input_ids'] = tokenizer(augment_text(feature['article']), max_length=1024, truncation=True)['input_ids']\n",
    "            if 'highlights' in feature:\n",
    "                feature['labels'] = tokenizer(feature['highlights'], max_length=128, truncation=True)['input_ids']\n",
    "        return super().__call__(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# åŠ è½½BARTåˆ†è¯å™¨å’Œæ¨¡å‹\n",
    "model_name = 'facebook/bart-large-cnn'\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# åˆå§‹åŒ– DataCollator\n",
    "data_collator = DataCollatorForSeq2SeqWithAugmentation(tokenizer, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# åŠ è½½ROUGEè¯„ä¼°æŒ‡æ ‡\n",
    "rouge = load_metric(\"rouge\", trust_remote_code=True)\n",
    "\n",
    "# å®šä¹‰è®¡ç®—æŒ‡æ ‡çš„å‡½æ•°\n",
    "def compute_metrics(p):\n",
    "    # å–å‡ºé¢„æµ‹çš„åºåˆ—\n",
    "    preds = p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "    preds = preds.argmax(-1)\n",
    "    \n",
    "    # è§£ç é¢„æµ‹çš„æ–‡æœ¬\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    \n",
    "    # å¤„ç†Noneå€¼ï¼Œé¿å…è§£ç é”™è¯¯\n",
    "    decoded_preds = [\"\".join([token if token is not None else \"\" for token in tokenizer.convert_ids_to_tokens(pred)]) for pred in preds]\n",
    "    \n",
    "    # è§£ç çœŸå®æ ‡ç­¾æ–‡æœ¬\n",
    "    decoded_labels = tokenizer.batch_decode(p.label_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # è®¡ç®—ROUGEåˆ†æ•°\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # è¿”å›æ‰€éœ€çš„ç»“æœ\n",
    "    result = {k: v.mid.fmeasure * 100 for k, v in result.items()}\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# è®¾ç½®è®­ç»ƒå‚æ•°\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bart_results',\n",
    "    logging_dir='./bart_logs',\n",
    "    warmup_steps=500,                 # é¢„çƒ­æ­¥æ•°\n",
    "    weight_decay=0.01,                # æƒé‡è¡°å‡\n",
    "    evaluation_strategy=\"epoch\",      # åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶è¿›è¡Œè¯„ä¼°\n",
    "    save_strategy=\"epoch\",            # åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ä¿å­˜æ¨¡å‹\n",
    "    # eval_steps=10,                   # æ¯éš”å¤šå°‘æ­¥è¿›è¡Œä¸€æ¬¡è¯„ä¼°\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=5,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=500,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# åˆå§‹åŒ–Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # ä½¿ç”¨è‡ªå®šä¹‰çš„DataCollator\n",
    "    # compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6250/6250 41:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.314600</td>\n",
       "      <td>1.758318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.904900</td>\n",
       "      <td>1.939801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.546800</td>\n",
       "      <td>2.237341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.313900</td>\n",
       "      <td>2.608653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.185200</td>\n",
       "      <td>2.878260</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0, 'forced_eos_token_id': 2}\n",
      "There were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation results: {'eval_loss': 1.7583175897598267, 'eval_runtime': 3.5504, 'eval_samples_per_second': 56.332, 'eval_steps_per_second': 14.083, 'epoch': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# å¼€å§‹è®­ç»ƒ\n",
    "trainer.train()\n",
    "\n",
    "# è®­ç»ƒå®Œæˆåè¯„ä¼°æ¨¡å‹\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Evaluation results: {eval_results}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä¸ºäº†å°½å¿«å®ç°è¿™ä¸ªpipelineï¼Œå°†è®­ç»ƒæ•°é‡è¿›è¡Œäº†ç¼©å‡ã€‚\n",
    "\n",
    "ä½ å¯ä»¥ä¿®æ”¹è¯»å–æ•°æ®é›†é‚£éƒ¨åˆ†çš„ä»£ç ï¼Œå®ç°å…¨é‡çš„è®­ç»ƒæ¥è¾¾åˆ°æ›´å¥½çš„æ•ˆæœã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
